<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: NLP | Carpe diem (Felix's blog)]]></title>
  <link href="http://www.idryman.org/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://www.idryman.org/"/>
  <updated>2012-06-17T16:22:28+08:00</updated>
  <id>http://www.idryman.org/</id>
  <author>
    <name><![CDATA[dryman (Felix Ren-Chyan Chern)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The NLP Day]]></title>
    <link href="http://www.idryman.org/blog/2012/03/14/the-nlp-day/"/>
    <updated>2012-03-14T19:55:00+08:00</updated>
    <id>http://www.idryman.org/blog/2012/03/14/the-nlp-day</id>
    <content type="html"><![CDATA[<p>Yesterday (3/13) was the NLP day, and we have the honor to invite Eduard Hovy to
give us three talks about the current status and future of NLP researches.
The talks were excellent! The first talk was about how machine learning perform
so well then previous AI systems. It seems that computer can be more intelligent
not because it is smart, but to be trained trained by huge amount of data. The
second talk was “A new semantics: merging propositional and distributional
information”. Eduard introduced a new model that combine theories and
computability that can be used in machine learning. The third talk was “Text
harvesting and ontology constructing using a powerful new method.” Using a
simple “$N_p \mbox{ such as } N_p \mbox{ and } N_p * $” query and search on Google
seems to be trivial, but the results are awesomely incredible! Thus we can mark
the “is-a” relationship automatically and the data is satisfying and robust,
plus, much more than any relationships in existing wordnets! There are so many
interesting topics to write down, but for mow, I’ll focus on the second talk
first. You can find the slide for the second talk 
<a href="http://projects.ict.usc.edu/rwt2011/presentations/hovy.pdf">here</a>.</p>

<!-- more -->

<p>Though machine learning told us that “you don’t have to be smart, you just need
enough training data,” deep in our heart, we still believe that all of our human
behaviors is not a huge training table. There must be some rules that guide us,
as a theory, to know what we will do and what wouldn’t. We believe that there
are theories that can measure info contents, not only treat input strings as
meaningless characters that the only purpose is to be sent to machine to do
statistical analysis.</p>

<h3 id="defining-a-concept-in-a-new-way">Defining a concept in a new way</h3>

<p>A concept $C$ is a list of triples</p>

<script type="math/tex; mode=display">
C=\left\{(r_1w_1s_1),(r_2w_2s_2),(r_3w_3s_3)\right\}
</script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
\mbox{where } &r_i\varepsilon\;\{Relations\}=&\mbox{e.g., :subj, :agent, :color-of}\\
& w_i\varepsilon\;\{Words\}=&\mbox{e.g., happy, run, apple}\\
& s_i\varepsilon\;[0,1]=&\mbox{normalized weight}
\end{align}
 %]]&gt;</script>

<p>For example, a dog can be represented as <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>{% codeblock lang:scheme %}
(defparameter <em>dog</em>
  ‘((:type “Jack Russel” 0.2)
    (:type “Retriever” 0.4)
    (:color “brown” 0.4)
    (:color “black” 0.3)
    (:agent-of “eat” 0.4)
    (:patient-of “chase” 0.3)))
{% endcodeblock %}</p>

<p>And it can be expressed in a more complexed form:</p>

<p>{% codeblock lang:scheme %}
(defparameter <em>dog</em>
  ‘((:type ((“Retriever” 0.4) (“Jack Russel” 0.2) (“Terrier” 0.4)))
    (:color ((“brown” 0.4) (“black” 0.3) (“patched” 0.3) (“white” 0.2)))
    (:name ((“Spot” 0.3) (“Lassie” 0.2)))
    (:agent-of ((“eat” 0.4) (“run” 0.4) (“bark” 0.4) (“pant” 0.3)))
    (:patient-of ((“chase” 0.3) (“walk” 0.4) (“love” 0.4 )))))
{% endcodeblock %}</p>

<p>which we can visualize it as below:
{% img /images/nlp-day/dog-context-1.png ‘An example of dog context’%}</p>

<p>Furthermore, since every node itself can be another context, we can generate a
graph! <sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>
{% img /images/nlp-day/dog-context-2.png ‘An another example of dog context’%}</p>

<h3 id="the-main-questions-to-address">The main questions to address</h3>

<ul>
  <li>Construction
    <ul>
      <li>Do we really need tensors? Or are vectors enough?</li>
      <li>How to build them?</li>
      <li>Which relations to use? Which scores?</li>
      <li>How do we evaluate this?</li>
    </ul>
  </li>
  <li>Compositionally: How to ‘add’ tensors to obtain new, more complex, meanings
that are still tensors?</li>
  <li>Dependency: How to represent and manage the underlying interconnections across
tensor elements?</li>
  <li>Logical operators: How to handle negation, quantification models (can,
must…), etc.?</li>
</ul>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>I use the syntax of common lisp to express the relationship<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The copyright of the images belongs to Eduard Hovy<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
