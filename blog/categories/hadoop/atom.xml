<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Carpe diem (Felix's blog)]]></title>
  <link href="http://www.idryman.org/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://www.idryman.org/"/>
  <updated>2017-05-07T21:14:58-07:00</updated>
  <id>http://www.idryman.org/</id>
  <author>
    <name><![CDATA[dryman (Felix Ren-Chyan Chern)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introducing Hadoop-FieldFormat]]></title>
    <link href="http://www.idryman.org/blog/2014/03/06/introducing-hadoop-fieldformat/"/>
    <updated>2014-03-06T14:25:00-08:00</updated>
    <id>http://www.idryman.org/blog/2014/03/06/introducing-hadoop-fieldformat</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/dryman/hadoop-fieldformat">Hadoop FieldFormat</a> is the new library I released that is flexible and robust for reading and setting schema information in Hadoop map-reduce program. We use this library to record the meta information for the data, and improve the semantic when building large map-reduce pipe-lined tasks. The project is quite stable now and we already used it in our production system. Any suggestion is welcome!</p>

<!--more-->

<h2 id="the-problem">The problem</h2>

<p>The map-reduce architecture is really good at aggregating large dataset and ad-hoc perform computation; however, when the number dataset increases, it becomes difficult to manage the meta data of those dataset. The biggest issue is data by default is semi-structured; there’s no schema or header information to tell you the semantic of the data. When working in raw map-reduce, this is typical code that I write:</p>

<p><code>java
void map (LongWritable keyIn, Text valIn, Context context) throws IOException, InterruptedException{
    String [] fields = valIn.toString().split("\\t");
    String ip = fields[0];
    String cookie = fields[1];
    String ua = fields[5];
    ...
}
</code></p>

<p>There’s no semantic associated with the data, so you can only hard code the semantic and hope the fields order will stay the same forever. If the upstream process inserted a new field to this dataset, your program may still run, but produce wrong result that might be difficult to catch by downstream program.</p>

<p>The same issue happens in <a href="https://pig.apache.org">Pig</a> and <a href="http://www.cascading.org">cascading</a> too. Pig, for example:</p>

<p><code>
tomcat = LOAD 'catalina.out' USING PigStorage('\t') AS (ip, cookie, query, url, time, ua);
</code></p>

<p>If the input format changed, you’ll need to be very careful to make sure all the downstream process are corrected. Moreover, if you want to run map-reduce across different versions of dataset, you may not be able to run it because the order of the fields is different!</p>

<h3 id="hive-and-hcatalog">Hive and HCatalog</h3>

<p>Goal: lightweight semantic attached to the data</p>

<h2 id="eat-our-own-dog-food----introducing-hadoop-fieldformat">Eat our own dog food – introducing Hadoop FieldFormat!</h2>

<p>You may be surprised by how simple the solution is. First, answer this:
Where does hadoop store the meta data for map-reduce jobs? <code>_logs</code>.</p>

<p>What hadoop FieldFormat does is reading and writing header.tsv. Also, provides
a convenient API in java to access the data field using the java Map interface.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop performance tuning best practices]]></title>
    <link href="http://www.idryman.org/blog/2014/03/05/hadoop-performance-tuning-best-practices/"/>
    <updated>2014-03-05T11:17:00-08:00</updated>
    <id>http://www.idryman.org/blog/2014/03/05/hadoop-performance-tuning-best-practices</id>
    <content type="html"><![CDATA[<p>I have been working on Hadoop in production for a while. Here are some of the performance tuning tips I learned from work. Many of my tasks had performance improved over 50% in general. Those guide lines work perfectly in my work place; hope it can help you as well.</p>

<!--more-->

<h2 id="tuning-hadoop-run-time-parameters">Tuning Hadoop run-time parameters</h2>

<p>Hadoop provides a set of options on cpu, memory, disk, and network for performance tuning. Most hadoop tasks are not cpu bounded, what we usually look into is to optimize usage of memory and disk spills.</p>

<h3 id="memory-tuning">Memory tuning</h3>

<p>The general rule for memory tuning is: use as much memory as you can, but don’t trigger swapping. The parameter you can set for task memory is <code>mapred.child.java.opts</code>. You can put it in your configuration file.</p>

<p>```xml</p>
<property>
    <name>mapred.child.java.opts</name>
    <value>-Xms1024M -Xmx2048M</value>
</property>
<p>```</p>

<p>You can tune the best parameters for memory by monitoring memory usage on server using Ganglia, Cloudera manager, or Nagios. Cloudera has a slide focused on memory usage tuning, the link is <a href="http://www.slideshare.net/Hadoop_Summit/optimizing-mapreduce-job-performance">here</a></p>

<h3 id="minimize-the-map-disk-spill">Minimize the map disk spill</h3>

<p>Disk IO is usually the performance bottleneck. There are a lot of parameters you can tune for minimizing spilling. What I use the most are:</p>

<ul>
  <li>compress mapper output</li>
  <li>Use 70% of heap memory for spill buffer in mapper</li>
</ul>

<p>In your configuration file, you can write:</p>

<p>```xml</p>
<property>
    <name>mapred.compress.map.output</name>
    <value>true</value>
</property>
<property>
    <name>mapred.map.output.compression.codec</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>
<property>
    <name>io.sort.mb</name>
    <value>800</value>
</property>
<p>```</p>

<p>Although you can further tune reducer buffer, mapper sort record percent, and various of stuff, I found the best thing to do is reduce the mapper output size. Most of the time, the performance is fast enough after I refactor the mapper to output as little data as possible. For more information, check the same <a href="http://www.slideshare.net/Hadoop_Summit/optimizing-mapreduce-job-performance">cloudera’s performance tuning guide</a>.</p>

<h3 id="tuning-mapper-tasks">Tuning mapper tasks</h3>

<p>Unlike reducer tasks which you can specify the number of reducer, the number of mapper tasks is set implicitly. The tuning goal for the mapper is control the amount of mapper and the size of each job. When dealing with large files, hadoop split the file in to smaller chunk so that mapper can run it in parallel. However, the initializing new mapper job usually takes few seconds, this is also a overhead that we want to minimize. These are the things you can do:</p>

<ul>
  <li>
    <p>Reuse jvm task</p>
  </li>
  <li>
    <p>If the average mapper running time is shorter than one minute, you can increase the <code>mapred.min.split.size</code>, so that less mappers are allocated in slot and thus reduces the mapper initializing overhead.</p>
  </li>
  <li>
    <p>Use Combine file input format for bunch of smaller files. I had an implementation that also use <code>mapred.min.split.size</code> to implicitly control the mapper size. You can find the <a href="https://github.com/dryman/Hadoop-CombineFileInputFormat">project on github</a>. The explanation of the project can be found on <a href="http://www.idryman.org/blog/2013/09/22/process-small-files-on-hadoop-using-combinefileinputformat-1/">my blog</a>.</p>
  </li>
</ul>

<p>The configuration file would look like this:</p>

<p>```xml</p>
<property>
    <name>mapred.job.reuse.jvm.num.tasks</name>
    <value>-1</value>
</property>
<property>
    <name>mapred.max.split.size</name>
    <value>268435456</value>
</property>
<property>
    <name>mapred.min.split.size</name>
    <value>134217728</value>
</property>
<p>```</p>

<h3 id="use-configuration-file-and-command-line-arguments-to-set-parameters">Use configuration file and command line arguments to set parameters</h3>

<p>When I first started on hadoop, I setup those parameters in java program, but it is so hard-coded and inflexible. Thankfully, hadoop provides <code>Tool</code> interface and <code>ToolRunner</code> class to parse those parameters for you. Here’s a sample program:</p>

<p>```java
public class ExampleJob extends Configured implements Tool{</p>

<p>public static void main (String[] args) throws Exception{
    System.exit(ToolRunner.run(new ExampleJob(), args));
  }</p>

<p>public int run(String[] args) throws Exception {
    Configuration conf = getConf();
    Job job = new Job(conf);
    // configure the rest of the job
  }
}
```</p>

<p>If your main class implements the interface, your program can take the config file as input:</p>

<p><code>bash
hadoop jar ExampleJob-0.0.1.jar ExampleJob -conf my-conf.xml arg0 arg1
</code></p>

<p>You can even pass extra parameters through command line like this:</p>

<p><code>bash
hadoop jar ExampleJob-0.0.1.jar ExampleJob -Dmapred.reduce.tasks=20 arg0 arg1
</code></p>

<p>Setting configuration as run-time arguments make you easier to test different parameters without recompile the program.</p>

<h2 id="tuning-application-specific-performance">Tuning application-specific performance</h2>

<p>Beyond general hadoop parameter setup, you can optimize your map-reduce program using some small tricks. Here are the tricks that I used the most.</p>

<h3 id="minimize-your-mapper-output">Minimize your mapper output</h3>

<p>Recall that mapper spill size is a serious performance bottleneck. The size of mapper output is sensitive to disk IO, network IO, and memory sensitive on shuffle phase. Minimizing the mapper output can improve the general performance a lot.</p>

<p>To do this, you can try the following</p>

<ol>
  <li>
    <p>Filter out records on mapper side, not on reducer side.</p>
  </li>
  <li>
    <p>Use minimal data to form your map output key and map output value.</p>
  </li>
  <li>
    <p>Extends <code>BinaryComparable</code> interface or use Text for your map output key</p>
  </li>
  <li>
    <p>Set mapper output to be compressed</p>
  </li>
</ol>

<p>Above all the optimization tips, I found this make the biggest change to many of my tasks, unless I can’t find a smaller key to reduce the mapper output. </p>

<h3 id="balancing-reducers-loading">Balancing reducer’s loading</h3>

<p>Another common performance issue that you might encounter is unbalanced reducer tasks: one or several reducer takes most of the output from mapper and ran extremely long compare to other reducers.</p>

<p>To solve this, you can either</p>

<ol>
  <li>
    <p>Implement a better hash function in <code>Partitioner</code> class.</p>
  </li>
  <li>
    <p>If you know what keys are causing the issue, you can write a preprocess job to separate keys using MultipleOutputs. Then use another map-reduce job to process the special keys that cause the problem.</p>
  </li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>It’s fun to write raw map-reduce jobs because it gives you more precise control over performance tuning. If you already experienced hive or pig, I encourage you to try how to optimize the same job using raw map-reduce. You can find a lot of performance gain and more space to tune the performance. For more curious, you can also check the <a href="http://www.slideshare.net/ydn/hadoop-summit-2010-tuning-hadoop-to-deliver-performance-to-your-application">Yahoo’s tuning hadoop performance guides</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Capture path info in hadoop InputFormat class]]></title>
    <link href="http://www.idryman.org/blog/2014/01/27/capture-path-info-in-hadoop-inputformat-class/"/>
    <updated>2014-01-27T15:11:00-08:00</updated>
    <id>http://www.idryman.org/blog/2014/01/27/capture-path-info-in-hadoop-inputformat-class</id>
    <content type="html"><![CDATA[<p>On the last post I presented how to use Mapper context object to obtain Path information. This is a nice way to hack for ad-hoc jobs; however, it’s not really reusable and abstract. In this post, I’ll show you how to subclass <code>Text</code>, <code>TextInputFormat</code>, and <code>LineRecordReader</code> and create reusable components across all of your hadoop tasks.</p>

<!--more-->

<h2 id="input-workflow">Input WorkFlow</h2>

<p>Before we go through all the classes, let me show you how hadoop read the files in.</p>

<ol>
  <li>
    <p>By default, hadoop uses <code>TextInputFormat</code>, which inherits <code>FileInputFormat</code>, to process the input files.</p>
  </li>
  <li>
    <p><code>TextInputFormat</code> allocates <code>LineRecordReader</code> and passed it to <code>Task</code> runtime.</p>
  </li>
  <li>
    <p><code>Task</code> first initiates <code>LineRecordReader</code>, then wrap the <code>LineRecordReader</code> into <code>Context</code> object.</p>
  </li>
  <li>
    <p>In <code>Mapper</code> <code>run</code> methods, it calls the method <code>nextKeyValue()</code> in <code>Context</code>, and reads the <code>LongWritable key</code> from <code>context.getCurrentKey()</code> and <code>Text value</code> from <code>context.getCurrentValue()</code>. Those methods delegates to <code>LineRecordReader</code>’s methods <code>nextKeyValue()</code>, <code>getCurrentKey()</code>, and <code>getCurrentValue()</code>.</p>
  </li>
  <li>
    <p>Finally, <code>Mapper</code> passes the key-value pair to <code>map</code> method that we usually overrides.</p>
  </li>
</ol>

<p>In order to put the path information into this workflow, we can extend the <code>Text</code> class and put the path information into it. To make this work, we need to create three new classes: <code>TextWithPath</code>, <code>TWPInputFormat</code>, and <code>TWPRecordReader</code>.</p>

<h2 id="textwithpathjava">TextWithPath.java</h2>

<p>Here is our content wrapper – <code>TextWithPath</code>. It doesn’t do much; there’s a new constructor which accepts <code>Path</code>, and there’s a getter method to get <code>Path</code>.</p>

<p>```java
package org.idryman;</p>

<p>import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;</p>

<p>public class TextWithPath extends Text {
  private Path path;</p>

<p>public TextWithPath(Path path){
    super();
    this.path = path;
  }</p>

<p>public Path getPath(){
    return path;
  }
}
```</p>

<h2 id="twpinputformatjava">TWPInputFormat.java</h2>

<p>The new <code>TWPInputFormat</code> is almost identical to <code>TextInputFormat</code>, except it uses <code>TextWithPath</code> instead of <code>Text</code>, and the <code>createRecordReader</code> method returns <code>TWPRecordReader</code> instead of <code>LineRecordReader</code>.</p>

<p>```java
package org.idryman;</p>

<p>import java.io.IOException;</p>

<p>import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.io.compress.SplittableCompressionCodec;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</p>

<p>public class TWPInputFormat extends FileInputFormat &lt;LongWritable, TextWithPath&gt;{
  @Override
  public RecordReader createRecordReader(InputSplit split,
      TaskAttemptContext context) throws IOException, InterruptedException {
    String delimiter = context.getConfiguration().get(
        “textinputformat.record.delimiter”);
    byte[] recordDelimiterBytes = null;
    if (null != delimiter)
      recordDelimiterBytes = delimiter.getBytes();
    return new TWPRecordReader(recordDelimiterBytes);
  }</p>

<p>@Override
  protected boolean isSplitable(JobContext context, Path file) {
    CompressionCodec codec = 
      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
    if (null == codec) {
      return true;
    }
    return codec instanceof SplittableCompressionCodec;
  }
}</p>

<p>```</p>

<h2 id="twprecordreaderjava">TWPRecordReader.java</h2>

<p>Finally, in the <code>TWPRecordReader</code>, this is where I put my logic in. In the <code>initialize</code> method, you can get the <code>FileSplit</code> and get the <code>Path</code> object out of it. Next, let’s override <code>nextKeyValue</code>, and updates the <code>value</code> on every call. Lastly, remember to override <code>getCurrentValue()</code>, else it will only return parent’s value (Text), not the value with <code>TextWithPath</code> class.</p>

<p>```java
package org.idryman;</p>

<p>import java.io.IOException;</p>

<p>import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;</p>

<p>public class TWPRecordReader extends LineRecordReader{
  private TextWithPath value = null;
  private Path path = null;</p>

<p>public TWPRecordReader(byte[] recordDelimiterBytes) {
    super(recordDelimiterBytes);
  }</p>

<p>@Override
  public void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException{
    super.initialize(genericSplit, context);
    FileSplit split = (FileSplit) genericSplit;
    path = split.getPath();
  }</p>

<p>@Override
  public boolean nextKeyValue() throws IOException {
    if (super.nextKeyValue()){
      if (value == null)
        value = new TextWithPath(path);
      value.set(super.getCurrentValue());
      return true;
    } else {
      value = null;
      return false;
    }
  }</p>

<p>@Override
  public TextWithPath getCurrentValue(){
    return value;
  }
}
```</p>

<h2 id="demo">Demo</h2>

<p>Here is a demo code to test the output. In addition to normal map reduce tasks, we set the input format class to <code>TWPInpuFormat</code>; on the Mapper side, we expect the input is <code>TextWithPath</code>, not <code>Text</code>. The whole program can be downloaded from this github repo. <a href="https://github.com/dryman/Hadoop-TextWithPath">Hadoop TextWithPath</a></p>

<p>```java
package org.idryman;</p>

<p>import java.io.IOException;</p>

<p>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;</p>

<p>public class DemoRun extends Configured implements Tool {</p>

<p>public static void main(String[] args) throws Exception {
    System.exit(ToolRunner.run(new Configuration(), new DemoRun(), args));
  }</p>

<p>@Override
  public int run(String[] args) throws Exception {
    Configuration conf = getConf();
    Job job = new Job(conf);
    job.setJobName(“test TextWithPath Input”);
    job.setJarByClass(DemoRun.class);</p>

<pre><code>TWPInputFormat.addInputPath(job, new Path(args[0]));
job.setInputFormatClass(TWPInputFormat.class);
job.setMapperClass(TestMapper.class);
job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(NullWritable.class);
job.setReducerClass(IntSumReducer.class);
job.setNumReduceTasks(1);
FileOutputFormat.setOutputPath(job, new Path(args[1]));

job.submit();
job.waitForCompletion(true);
return 0;   }
</code></pre>

<p>public static class TestMapper extends Mapper&lt;LongWritable, TextWithPath, Text, IntWritable&gt;{</p>

<pre><code>/**
 * Only override `run` instead of `map` method; because we just want to see one output
 * per mapper, instead of printing every line.
 */
@Override
public void run(Context context) throws IOException, InterruptedException{
  context.nextKeyValue();
  TextWithPath twp = context.getCurrentValue();
  context.write(new Text(twp.getPath().toString()), new IntWritable(1));
}   }
</code></pre>

<p>}
```</p>

<h2 id="one-more-thing">One more thing</h2>

<p>I wrote another hadoop utility that reads a header file from HDFS input source, and passes a <code>FieldWritable</code> object to <code>Mapper</code> class instead of <code>Text</code>. The <code>FieldWritable</code> implements <code>Map</code> interface and can obtain TSV fields by it’s header key. The project is on <a href="https://github.com/dryman/hadoop-fieldformat">github</a> but still highly experimental. Once the API and implementation is stable, I’ll write another post to introduce it. Enjoy!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Capture directory context in Hadoop Mapper]]></title>
    <link href="http://www.idryman.org/blog/2014/01/26/capture-directory-context-in-hadoop-mapper/"/>
    <updated>2014-01-26T15:12:00-08:00</updated>
    <id>http://www.idryman.org/blog/2014/01/26/capture-directory-context-in-hadoop-mapper</id>
    <content type="html"><![CDATA[<p>I have been using hadoop for data processing and datawarehousing for a while. One of the problem we encountered was map-reduce framework abstracts the input from files to lines, and thus it’s really difficult to apply logic based on different file or directories. Things got worse when we need to aggregate data across various versions of input sources. After digging in Hadoop source code, here is my solution.</p>

<!--more-->

<h2 id="meets-inputsplit-in-mapper">Meets InputSplit in Mapper</h2>

<p>Here is the first solution, it’s a bit ugly but works. In Hadoop <code>Mapper</code> class, you can override the <code>setup</code> method to initiate mapper with corresponding context. The context contains <code>inputSplit</code>, which can be cast to <code>FileInputSplit</code>, and contains the file and directory information in it. This is how I did it:</p>

<p><code>java
protected void setup(Context context) throws IOException{
  FileSplit fileSplit;
  InputSplit is = context.getInputSplit();
  FileSystem fs = FileSystem.get(context.getConfiguration());
  fileSplit = (FileSplit) is;
  Path filePath = fileSplit.getPath();
}
</code></p>

<p>If the input directory looks like this:</p>

<p><code>
/input/part-r-00000
       part-r-00001
       part-r-00002
       part-r-00003
</code></p>

<p>And the path argument you passed to <code>FileInputFormat</code> is <code>/input</code>. The resulting paths in the snippet would be one of these:</p>

<p><code>
/input/part-r-00000
/input/part-r-00001
/input/part-r-00002
/input/part-r-00003
</code></p>

<p>Each Mapper would get different <strong>file</strong> path instead of getting the directory <code>/input</code>. If you want to handle the logic better, you can do this:</p>

<p><code>java
Path finalPath;
if (fs.isFile(filePath)){
  finalPath = new Path(filePath.getParrent());
} else {
  finalPath = filePath;
}
</code></p>

<h2 id="taggedinputsplit">TaggedInputSplit</h2>

<p>This works for most of the time; however, if you set the input to be <code>/input/*/part*</code>. The <code>InputSplit</code> would be an internal type called <code>TaggedInputSplit</code> instead of <code>FileInputSplit</code>. Although <code>TaggedInputSplit</code> has a method called <code>getInputSplit</code> to get the wrapped class, it is a private class and you can only use java reflection to hack it.</p>

<p><code>
InputSplit is = context.getInputSplit();
Method method = is.getMethod("getInputSplit");
method.setAccessible(true);
fileSplit = (FileSplit) method.invoke(is);
Path filePath = fileSplit.getPath();
</code></p>

<h2 id="more-general-solutions">More general solutions</h2>

<p>The solutions above is working on production environment. However, it is a bit too hacky and not general enough. On the next post, I’ll show you how to implement <code>InputFomrat</code>, <code>RecordReader</code>, and <code>Writable</code> classes to solve this problem with lower level APIs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Process Small Files on Hadoop using CombineFileInputFormat (2)]]></title>
    <link href="http://www.idryman.org/blog/2013/09/22/process-small-files-on-hadoop-using-combinefileinputformat-2/"/>
    <updated>2013-09-22T18:41:00-07:00</updated>
    <id>http://www.idryman.org/blog/2013/09/22/process-small-files-on-hadoop-using-combinefileinputformat-2</id>
    <content type="html"><![CDATA[<p>Followed the previous article, in this post I ran several benchmarks and tuned the performance from 3 hours 34 minutes to 6 minutes 8 seconds!</p>

<!--more-->

<h2 id="original-job-without-any-tuning">Original job without any tuning</h2>

<ul>
  <li><code>job_201308111005_0317</code></li>
  <li>NumTasks: 9790</li>
  <li>Reuse JVM: false</li>
  <li>mean complete time: 9-Sep-2013 10:08:47 (17sec)</li>
  <li>Finished in: 3hrs, 34mins, 26sec</li>
</ul>

<p>We had 9790 files to process, and the total size of the files is 53 GB. Note that for every task it still took 17 seconds to process the file.</p>

<h2 id="using-combinefileinputformat-without-setting-the-maxsplitsize">Using CombineFileInputFormat without setting the MaxSplitSize</h2>

<ul>
  <li><code>job_201308111005_0330</code></li>
  <li>NumTasks: 1</li>
  <li>Reuse JVM: false</li>
</ul>

<p>In this benchmark I didn’t set the <code>MaxSplitSize</code> in <code>CFInputFormat.java</code>, and thus Hadoop merge all the files into one super big task.
After running this task for 15 minutes, hadoop killed it. Maybe its a timeout issue, I didn’t dig into this.
The start and the end of the task logs look like this:</p>

<pre><code>13/09/09 16:17:29 INFO mapred.JobClient:  map 0% reduce 0%
13/09/09 16:32:45 INFO mapred.JobClient:  map 40% reduce 0%
 
13/09/09 16:33:02 INFO mapred.JobClient: Task Id : attempt_201308111005_0330_m_000000_0, Status : FAILED
java.lang.Throwable: Child Error
    at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
    Caused by: java.io.IOException: Task process exit with nonzero status of 255.
    at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)
</code></pre>

<h2 id="using-combinefileinputformat-with-block-size-64-mb">Using CombineFileInputFormat with block size 64 MB</h2>

<ul>
  <li><code>job_201308111005_0332</code></li>
  <li>Reuse JVM = false</li>
  <li>max split size = 64MB</li>
  <li>NumTasks: 760</li>
  <li>mean complete time: 9-Sep-2013 16:55:02 (24sec)</li>
  <li>Finished in: 23mins, 6sec</li>
</ul>

<p>After modifying <code>MaxSplitSize</code> the total runtime has reduced to 23 minutes! The total tasks drops from 9790 to 760, about 12 times smaller. The time difference is 9.3 times faster, pretty nice! However, the mean complete time doesn’t scale like other factors. The reason was it’s a big overhead to start JVM over and over again.</p>

<h2 id="using-combinefileinputformat-with-block-size-64mb-and-reuse-jvm">Using CombineFileInputFormat with block size 64MB and reuse JVM</h2>

<p>To reuse the JVM, just set <code>mapred.job.reuse.jvm.tasks</code> to <code>-1</code>. </p>

<p><code>java
  public static void main(String[] argv) throws Exception {
    Configuration conf = new Configuration();
    conf.setInt("mapred.job.reuse.jvm.num.tasks", -1);
    int res = ToolRunner.run(conf, new HadoopMain(), argv);
    System.exit(res);
  }
</code></p>

<p>The result is awesome! <strong>6 minutes and 8 seconds</strong>, wow!</p>

<ul>
  <li><code>job_201308111005_0333</code></li>
  <li>Reuse JVM = true</li>
  <li>max split size = 64MB</li>
  <li>NumTasks: 760</li>
  <li>mean complete time: 9-Sep-2013 17:30:23 (5sec)</li>
  <li>Finished in: 6mins, 8sec</li>
</ul>

<h2 id="use-fileinputformat-and-reuse-jvm">Use FileInputFormat and reuse JVM</h2>

<p>Just curious the performance difference if we only change the JVM parameter:</p>

<ul>
  <li><code>job_201308111005_0343 </code></li>
  <li>NumTasks: 9790</li>
  <li>mean complete time: 10-Sep-2013 17:04:18 (3sec)</li>
  <li>Reuse JVM = true</li>
  <li>Finished in: 24mins, 49sec</li>
</ul>

<h2 id="tuning-performance-over-block-size">Tuning performance over block size</h2>

<p>Let’s jump to the conclusion first: changing the block size doesn’t affect the performance that much, and I found 64 MB is the best size to use. Here are the benchmarks:</p>

<h3 id="mb">512 MB</h3>

<ul>
  <li><code>job_201308111005_0339</code></li>
  <li>Reuse JVM = true</li>
  <li>max split size = 512MB</li>
  <li>NumTasks: 99</li>
  <li>mean complete time: 10-Sep-2013 11:55:26 (24sec)</li>
  <li>Finished in: 7min 13sec</li>
</ul>

<h3 id="mb-1">128 MB</h3>

<ul>
  <li><code>job_201308111005_0340</code></li>
  <li>Reuse JVM = true</li>
  <li>max split size = 128 MB</li>
  <li>NumTasks: 341</li>
  <li>mean complete time: 10-Sep-2013 13:13:20 (9sec)</li>
  <li>Finished in: 6mins, 41sec</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>So far the best practice I learned from these benchmarks are:</p>

<ol>
  <li>Setup the <code>mapred.job.reuse.jvm.num.tasks</code> flag in configuration. This is the easiest tuning to do, and it makes nearly 10 times performance improvement.</li>
  <li>Write your own <code>CombineFileInputFormat</code> implementation.</li>
  <li>The block size can be 64 MB or 128 MB, but doesn’t make big difference between the two.</li>
</ol>

<p>Still, try to model your problems into sequence file or map file in hadoop. HDFS should handle localities with these files automatically.
What about <code>CFInputFormat</code>? Does it handle locality in HDFS system too?
I can’t confirm it but I guess sorting the keys based on line offset first then file name also guarantees the locality of assigning data to mapper. When I have time to dig more from HDFS API, I’ll look back to this benchmark and see what can I further tune the program.</p>
]]></content>
  </entry>
  
</feed>
