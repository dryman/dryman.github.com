<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hash | Carpe diem (Felix's blog)]]></title>
  <link href="http://www.idryman.org/blog/categories/hash/atom.xml" rel="self"/>
  <link href="http://www.idryman.org/"/>
  <updated>2017-07-19T09:24:07-07:00</updated>
  <id>http://www.idryman.org/</id>
  <author>
    <name><![CDATA[dryman (Felix Ren-Chyan Chern)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[learn hash table the hard way -- part 2: probe distribution with deletions]]></title>
    <link href="http://www.idryman.org/blog/2017/07/18/learn-hash-table-the-hard-way-2/"/>
    <updated>2017-07-18T19:02:00-07:00</updated>
    <id>http://www.idryman.org/blog/2017/07/18/learn-hash-table-the-hard-way-2</id>
    <content type="html"><![CDATA[<p>In the last post I demonstrated probe distributions under six insertion
schemes: linear probing, quadratic probing, double hashing, and their
robin hood hashing variants. Knowing the probe distribution of insertion
is useful for static hash tables, but does not model the hash table with
frequent updates. Therefore I made some experiments for such scenario,
and found the results are very interesting.</p>

<!--more-->

<hr />

<p>Same disclaimer. I now work at google, and this project
(OPIC including the hash table implementation) is approved by google
<a href="https://opensource.google.com/docs/iarc/">Invention Assignment Review Committee</a> as my personal
project. The work is done only in my spare time on my own machine,
and does not use and/or reference any of the google internal resources.</p>

<h2 id="deletion-in-open-addressing-schemes">Deletion in open addressing schemes</h2>

<p>In most open addressing schemes, deletion is done by marking the
bucket with a tombstone flag. During the next insertion, both
empty bucket and tombstone bucket can hold new items. During
look ups, seeing an empty bucket means the key was not found, but if
you saw a tombstone bucket you must continue probing. If too many
items got deleted and causes the load smaller than a threshold,
shrink the hash table and re-insert the non-tombstone items.</p>

<h2 id="table-with-high-insertiondeletion-rate">Table with high insertion/deletion rate</h2>

<p>Consider the following: you are maintaining a large key value store
which uses hash table internally. The key value store has many
keys inserted and deleted frequently, but the total keys remains the
same (limited by capacity or ttl). Let’s assume all keys have equal
probability to get deleted. The keys with low probing count would
eventually get deleted at sometime, while the newly inserted key
may occur with high probing count because the table is always
under high load. An interesting question rises:</p>

<ul>
  <li>Will the probe number continue growing?</li>
  <li>Or the probe number converges to certain distribution?</li>
</ul>

<p>I yet to see mathematical analysis on this problem. If you know a good
reference, please leave a comment. Finding the formal bound were too
hard for me, so I designed a small experiment to understand the
effect. The experiment will have ten rounds. In the first round,
insert 1M items. In the next nine rounds, delete an item and insert a
new item for 1M times. I only tested this experiment on quadratic
probing scheme and robin hood with quadratic probing.</p>

<h3 id="quadratic-probing-with-deletion">Quadratic probing with deletion</h3>

<p>Quadratic probing is used in <a href="http://goog-sparsehash.sourceforge.net/doc/dense_hash_map.html">dense hash map</a>. This is one of the
fastest hash table with wide adoption, therefore worth the study.
For this experiment I didn’t use <a href="http://goog-sparsehash.sourceforge.net/doc/dense_hash_map.html">dense hash map</a>, instead I wrote
a small C program with same probing algorithm and record the probe counts.
The chart below is a histogram of probe count for quadratic probing.
Each line is the distribution of probes of different rounds; <code>00</code> is
insertion only round, and others have pair of deletion and insert.
Each round have 1M items inserted and/or deleted. The table is under
80% load.</p>

<p><img src="/images/del_quadratic_str.png" alt="del quadratic" /></p>

<p>Surprisingly, the probe histogram converges to a shape after
one round. This means that the hash table performance will
drop after one round of replacing all the elements, but will reach
to a steady state and stop getting worse. The shape of the steady
distribution looks like a exponential distribution. I wonder can
we use this property and further derive other interesting properties?</p>

<h3 id="robin-hood-hash-with-deletion">Robin hood hash with deletion</h3>

<p>In the <a href="https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf">robin hood hashing thesis</a> the author conjectured
that having deletion would cause the mean of probe count increase
without bound, but the variance would remain bounded by small constant.</p>

<p>[Paul Khuong][rhh del2] and <a href="http://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/">Emmanuel Goossaert</a> pioneered
to approach this problem. The intuition is fill the deleted bucket
by scanning forward candidate buckets. See <a href="http://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/">Emmanuel’s post</a>
for more detail.</p>

<p>[rhh del2: https://www.pvk.ca/Blog/more_numerical_experiments_in_hashing.html</p>

<p>Inspired by their robin hood linear probing deletion, I created one
for robin hood quadratic probing. The idea is similar, except the
candidates are not limited to its neighbors. I have to scan through
possible candidates from largest probe number, and check is the
candidate valid to fill the spot. There are some other tricks I did
to make sure the iteration done in deletion is bounded, but isn’t
important in this post.</p>

<p>The probe distribution using this idea is shown as follows:</p>

<p><img src="/images/rhh_del_str.png" alt="rhh del str" /></p>

<p>The result is also very good. Both the mean and variance is smaller
than naive quadratic probing. Luckily, the conjecture of unbounded
mean wasn’t true, it converges to a certain value! Recall from last
post; we want to know what is the worst case probe (&lt; 20 for 1M inserts)
and the average case. Even with lots of inserts and deletes, the
mean is still in constant bound, and the worst case is not larger than
O(log(N)).</p>

<p>How about robin hood hashing without the re-balancing strategy? Again,
the results blows my mind:</p>

<p><img src="/images/rhh_del_str2.png" alt="rhh del str2" /></p>

<p>It’s actually very identical to my carefully designed deletion method.
When I first see the experiment result, I was quite shocked.  I can do
nothing but to accept the experiment result, and adapt new
implementation.  In my journey of optimizing hash tables, I found
clever ideas often failed (but not always!). Finding a good
combination of naive and clever ideas for good performance is tough. I
did it by doing exhaustive search of different combinations, then
carefully measure and compare.</p>

<p>In <a href="http://opic.rocks/struct_robin_hood_hash%E3%80%80.html">OPIC robin hood hashing</a> I initially only interested at
building static hash table with high load. However, after this
experiments I concluded that robin hood hashing has good potential for
dynamic hash table as well.</p>

<h2 id="aggregated-stats">Aggregated stats</h2>

<p>Last but not least, let’s look at mean and variance for each method
and each round.</p>

<p><img src="/images/del_stat.png" alt="del stat" /></p>

<p>The mean of quadratic probing and robin hood quadratic probing actually
doesn’t differ by much. Only a little bit after first round. The difference
of variance is huge because that’s what robin hood hashing is designed for.</p>

<h2 id="summary">Summary</h2>

<p>In the first two post of learn hash table series, we examined probe
distributions of various methods and scenarios. In the next post I’ll
show how these distribution reflects on actual performance. After all,
these experiments and study were meant to leads to better engineering
result.</p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf">Robin hood probing thesis</a></li>
  <li><a href="http://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/">Emmanuel’s back ward shift deletion</a></li>
  <li><a href="http://opic.rocks/struct_robin_hood_hash%E3%80%80.html">OPIC robin hood quadratic probing</a></li>
  <li><a href="http://goog-sparsehash.sourceforge.net/doc/dense_hash_map.html">dense hash map</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[learn hash table the hard way -- part 1: probe disributions]]></title>
    <link href="http://www.idryman.org/blog/2017/07/04/learn-hash-table-the-hard-way/"/>
    <updated>2017-07-04T13:05:00-07:00</updated>
    <id>http://www.idryman.org/blog/2017/07/04/learn-hash-table-the-hard-way</id>
    <content type="html"><![CDATA[<p>In the last 4 months I’ve been working on how to implement a good hash table
for <a href="https://github.com/dryman/opic">OPIC (Object Persistence in C)</a>. During the development, I made
a lot of experiments. Not only for getting better performance, but also knowing
deeper on what’s happening inside the hash table. Many of these findings are
very surprising and inspiring. Since my project is getting mature, I’d get
a pause and start writing a hash table deep dive series. There was a lot of
fun while discovering these properties. Hope you enjoy it as I do.</p>

<!--more-->

<p>Same disclaimer. I now work at google, and this project
(OPIC including the hash table implementation) is approved by google
<a href="https://opensource.google.com/docs/iarc/">Invention Assignment Review Committee</a> as my personal
project. The work is done only in my spare time on my own machine,
and does not use and/or reference any of the google internal resources.</p>

<h2 id="background">Background</h2>

<p>Hash table is one of the most commonly used data structure. Most standard
library use <a href="https://en.wikipedia.org/wiki/Hash_table#Separate_chaining">chaining</a> hash table, but there are more options in
the wild. In contrast to <a href="https://en.wikipedia.org/wiki/Hash_table#Separate_chaining">chaining</a>, <a href="https://en.wikipedia.org/wiki/Open_addressing">open addressing</a> does
not create a linked list on bucket with collision, it insert the item
to other bucket instead. By inserting the item to nearby bucket, open
addressing gains better cache locality and is proven to be faster in many
benchmarks. The action of searching through candidate buckets for insertion,
look up, or deletion is known as <em>probing</em>. There are many probing strategies:
<a href="https://en.wikipedia.org/wiki/Linear_probing">linear probing</a>, <a href="https://en.wikipedia.org/wiki/Quadratic_probing">quadratic probing</a>, <a href="https://en.wikipedia.org/wiki/Double_hashing">double hashing</a>, <a href="https://en.wikipedia.org/wiki/Hash_table#Robin_Hood_hashing">robin
hood hasing</a>, <a href="https://en.wikipedia.org/wiki/Hopscotch_hashing">hopscotch hashing</a>, and <a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">cuckoo hashing</a>.
Our first post is to examine and analyze the probe distribution among these
strategies.</p>

<p>To write a good open addressing table, there are several factors to consider:
1. load: load is the number of bucket occupied over the bucket
   capacity. The higher the load, the better the memory utilization is.
   However, higher load also means the probability to have collision is higher.
2. probe numbers: the number of probes is the number of look up to reach the
   desired items. Regardless of cache efficiency, the lower the total probe
   count, the better the performance is.
3. CPU cache hit and page fault: we can count both the cache hit and page
fault analytically and from cpu counters. I’ll write such analysis in later
post.</p>

<h2 id="linear-probing-quadratic-probing-and-double-hashing">Linear probing, quadratic probing, and double hashing</h2>

<p>Linear probing can be represented as a hash function of a key and a
probe number $h(k, i) = (h(k) + i) \mod N$. Similarly, quadratic
probing is usually written as $h(k, i) = (h(k) + i^2) \mod N$.  Double
hashing is defined as $h(k, i) = (h1(k) + i \cdot h2(k)) \mod N$.</p>

<p>Quadratic probing is used by <a href="http://goog-sparsehash.sourceforge.net/doc/dense_hash_map.html">dense hash map</a>. In my knowledge
this is the fastest hash map with wide adoption. Dense hash map set
the default maximum load to be 50%. Its table capacity is bounded
to power of 2. Given a table size $2^n$, insert items $2^{n-1} + 1$,
you can trigger a table expansion, and now the load is 25%. We can
claim that if user only insert and query items, the table load is
always within 25% and 50% (the table may need to expand at least once).</p>

<p>I implemented a <a href="https://github.com/dryman/opic/blob/master/benchmark/robin_hood/generic_table.c">generic hash table</a> to simulate dense hash
map probing behaviors. Its performance is identical to dense hash
map. The major difference is I allow non power of 2 table size, see
<a href="http://www.idryman.org/blog/2017/05/03/writing-a-damn-fast-hash-table-with-tiny-memory-footprints/">my previous post</a> for why the performance does not degrade.</p>

<p>I setup the test with 1M inserted items. Each test differs in its load
(by adjusting the capacity) and probing strategies.
Although hash table is O(1) on amortized look up, we’ll still hope the
worst case not larger than O(log(N)), which is log(1M) = 20 in this case.
Let’s first look at linear probing, quadratic
probing and double hashing under 30%, 40%, and 50% load.</p>

<p><img src="/images/low_load.png" alt="low load" /></p>

<p>This is a histogram of probe counts. The Y axis is log scale. One can
see that other than linear probing, most probes are below 15. Double
hashing gives us smallest probe counts, however each of the probe has
high probability trigger a cpu cache miss, therefore is slower in
practice.  Next, we look at these methods under high load.</p>

<p><img src="/images/high_load.png" alt="high load" /></p>

<p>The probe distribution now have a very high variance. Obviously, many
probes exceeds the 20 threshold, some even reach 800.
Linear probing, among the other methods, has very bad variance under
high load. Quadratic probing is slightly better, but still have some
probes higher than 100.  Double hashing still gives the best probe
statistics. Below is the zoom in for each probe strategies:</p>

<p><img src="/images/lp_high_load.png" alt="linear probe high load" /></p>

<p><img src="/images/qp_high_load.png" alt="quadratic probe high load" /></p>

<p><img src="/images/dh_high_load.png" alt="double hashing high load" /></p>

<h2 id="robin-hood-hashing-for-the-rescue">Robin Hood Hashing for the rescue</h2>

<p>The robin hood hashing heuristic is simple and clever. When
a collision occur, compare the two items’ probing count, the one
with larger probing number stays and the other continue to probe.
Repeat until the probing item finds an empty spot. For more detailed
analysis checkout <a href="https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf">the original paper</a>.
Using this heuristic, we can reduce the variance dramatically.</p>

<p><img src="/images/rhh_high_load.png" alt="robin hood high load" /></p>

<p>The linear probing now have the worst case not larger than 50,
quadratic probing has the worst case not larger than 10, and
double hashing has the worst case not larger than 5! Although
robin hood hashing adds some extra cost on insert and deletion,
but if your table is read heavy, it’s really suitable for the job.</p>

<h2 id="dive-deep-and-ask-why">Dive deep and ask why</h2>

<p>From engineering perspective, the statistics are sufficient to make
design decisions and move on to next steps (though, hopscotch and
cuckoo hashing was not tested). That what I did 3 months ago. However,
I could never stop asking why. How to explain the differences? Can
we model the distribution mathematically?</p>

<p>The analysis on linear probing can trace back to 1963 by Donald Knuth.
(It was an unpublished memo dated July 22, 1963. With annotation “My
first analysis of an algorithm, originally done during Summer 1962 in
Madison”). Later on the paper worth to read are:</p>

<ul>
  <li><a href="https://pdfs.semanticscholar.org/8a08/7f5d581f9992936c7c73269c52138a63a3c3.pdf">Svante Janson, 2003, INDIVIDUAL DISPLACEMENTS FOR LINEAR PROBING HASHING
WITH DIFFERENT INSERTION POLICIES</a></li>
  <li><a href="http://www.kurims.kyoto-u.ac.jp/EMIS/journals/DMTCS/pdfpapers/dmAD0127.pdf">Alfredo Viola, 2005, Distributional analysis of Robin Hood linear probing
 hashing with buckets</a></li>
  <li><a href="https://dmtcs.episciences.org/519/pdf">Alfredo Viola, 2010, Distributional Analysis of the Parking Problem and
 Robin Hood Linear Probing Hashing with Buckets</a></li>
</ul>

<p>Unfortunately, these research are super hard. Just linear probing (and its
robin hood variant) is very challenging. Due to my poor survey ability, I
yet to find a good reference to explain what causes linear probing, quadratic
probing and double hashing differ on the probe distribution. Though building
a full distribution model is hard, but creating a simpler one to convince myself
turns out is not too hard.</p>

<h2 id="rich-get-richer">Rich get richer</h2>

<p>The main reason why linear probing (and probably quadratic probing) gets high
probe counts is rich get richer: if you have a big chunk of elements, they
are more likely to get hit; when they get hit, the size of the chunk grows,
and it just get worse.</p>

<p>Let’s look at a simplified case. Say the hash table only have 5 items, and all
the items are in one consecutive block. What is the expected probing number for
the next inserted item?</p>

<p><img src="/images/linear_probe_exp.png" alt="linear probe" /></p>

<p>See the linear probing example above. If the element get inserted to bucket 1,
it has to probe for 5 times to reach the first empty bucket. (Here we start the
probe sequence from index 0; probe number = 0 means you inserted to an empty
spot without collision). The expectation probing number for next inserted item
is</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
\sum \mathrm{probe} \cdot p & = 5\cdot\frac{1}{N} +
4\cdot\frac{1}{N} + 3\cdot\frac{1}{N} + 2\cdot\frac{1}{N} +
1\cdot\frac{1}{N} + 0\cdot\frac{1}{N}(N-5) \\
& = \frac{5+4+3+2+1}{N} \\
& = \frac{15}{N}
\end{align}
 %]]&gt;</script>

<p>For quadratic probing, you’ll have to look at each of the item and track
where it first probe outside of the block.</p>

<p><img src="/images/quadratic_probe_exp.png" alt="Quadratic probe" /></p>

<p>The expected probe number for next item in quadratic probing is
$\frac{3+2+2+2+1}{N} = \frac{10}{N}$. Double hashing is the easiest:
$1\cdot\frac{5}{N}+2\cdot(\frac{5}{N})^2+3\cdot(\frac{5}{N})^3+\cdots$
If we only look at the first order (because N » 5), then we can
simplify it to $\frac{5}{N}$.</p>

<ul>
  <li>Linear probing: $\frac{15}{N}$</li>
  <li>Quadratic probing: $\frac{10}{N}$</li>
  <li>Double hashing: $\sum_{i=1} i\cdot(\frac{5}{N})^i$</li>
</ul>

<p>The expected probe number of next item shows that linear probing is
worse than other method, but not by too far. Next, let’s look at
what is the probability for the block to <em>grow</em>.</p>

<p><img src="/images/lp_grow.png" alt="linear grow" /></p>

<p><img src="/images/qp_grow.png" alt="quadratic grow" /></p>

<p>To calculate the probability of the block to grow on next insert, we
have to account the two buckets which connected to the block. For linear
probing, the probability is $\frac{5+2}{N}$. For quadratic probing, we
add the connected block, but we also have to remove the buckets which
would jump out during the probe. For double hashing, the probability
to grow the block has little to do with the size of the block, because
you only need to care the case where it inserted to the 2 connected
buckets.</p>

<ul>
  <li>Linear probing: $\frac{7}{N}$</li>
  <li>Quadratic probing: $\frac{4}{N}$</li>
  <li>Double hashing: $\frac{2}{N}\cdot\sum_{i=0}(\frac{5}{N})^i =
\frac{2}{N}\cdot\frac{N}{N-5} = \frac{2}{N-5}$</li>
</ul>

<p>Using the same calculation, but making the block size as a variable,
we can now visualize the block growth of linear probing, quadratic
probing, and double hashing.</p>

<p><img src="/images/block_grow.png" alt="block grow" /></p>

<p>This is not a very formal analysis. However, it gives us a sense of why
the rate of linear probing getting worse is way larger than the others.
Not only knowing which one is better than the other, but also knowing
how much their differences are.</p>

<p>How about the robin hood variant of these three probing methods?
Unfortunately, I wasn’t able to build a good model that can explain
the differences. A formal analysis on robin hood hashing using linear
probing were developed by <a href="http://www.kurims.kyoto-u.ac.jp/EMIS/journals/DMTCS/pdfpapers/dmAD0127.pdf">Viola</a>. I yet to find a good analysis
for applying robin hood on other probing method. If you find it, please
leave a comment!</p>

<h2 id="conclusion">Conclusion</h2>

<p>Writing a (chaining) hash table to pass an interview is trivial, but writing
a good one turns out to be very hard. The key for writing high performance
software, is <em>stop guessing</em>.</p>

<p>Measure, measure, and measure. Program elapsed time is just one of the
sample point, and can be biased by many things. To understand the
program runtime performance, we need to further look at program
internal statistics (like probe distribution in this article), cpu
cache misses, memory usage, page fault count, etc. Capture the
information, and analyze it scientifically.  This is the only way to
push the program to its limit.</p>

<p>This my first article of “Learn hash table the hard way” series. In
the following post I’ll present more angles on examining hash table performance.
Hope you enjoy it!</p>
]]></content>
  </entry>
  
</feed>
